{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Data Wrangling\n",
    "*Kimberly Kaufman*  \n",
    "*June 15, 2019*\n",
    "  \n",
    "---\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The following steps were taken to clean and wrangle one year's worth of outbound distribution and freight cost data related to Wiley's North American distribution center. This data has been extracted from a combination of proprietary internal systems (via SQL) and archived csv files of monthly freight bills. The full dataset will be used to predict the future freight costs of physical book fulfillment through a supervised machine learning approach.\n",
    "\n",
    "For more details on this project, see the <a href=\"https://github.com/kaufkauf/Capstone-Project-Intermediate\">GitHub repository</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "\n",
    "The data for this project is available as 5 separate csv files.  Four of these files contain 3 months' each (or four quarters' total) worth of outbound volume data (read in below as \"vol\") and the fifth csv file contains 12 months' worth of freight cost data from Wiley's 3PL provider (read in below as \"freight\").  Both the consolidated volume and freight datasets represent the same 12 month time period of ship verify and 3PL interface dates, respectively, for May 2018 through April 2019 (as this represents Wiley's fiscal reporting year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1131645 entries, 0 to 280156\n",
      "Data columns (total 22 columns):\n",
      "CJDAT1                1131645 non-null int64\n",
      "IHINDT                1131645 non-null int64\n",
      "IHAEDT                1131645 non-null int64\n",
      "INVOICENO             1131645 non-null int64\n",
      "ISBN10                1131645 non-null object\n",
      "PRODUCT_LINE          1131645 non-null object\n",
      "GLOBAL_BUSINESS       1131645 non-null object\n",
      "BOOK_WEIGHT           1131645 non-null float64\n",
      "COLLECT_METHOD        1131645 non-null object\n",
      "CARRIER_CODE          1131645 non-null object\n",
      "PRIORITY_CODE         1131645 non-null object\n",
      "SHIP_METHOD           1131645 non-null object\n",
      "FREIGHT_TYPE          1048609 non-null object\n",
      "SHIP_COUNTRY_NAME     1131632 non-null object\n",
      "MARKET_OUTLET         1131645 non-null int64\n",
      "BATCH_NO              1131645 non-null object\n",
      "CARTON_QTY            1131645 non-null int64\n",
      "CARTONS_PER_PALLET    1131645 non-null int64\n",
      "TOTAL_UNITS           1131645 non-null int64\n",
      "TOTAL_LOOSE           1131645 non-null int64\n",
      "TOTAL_CARTONS         1131645 non-null int64\n",
      "TOTAL_PALLETS         1131645 non-null int64\n",
      "dtypes: float64(1), int64(11), object(10)\n",
      "memory usage: 198.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129822 entries, 0 to 129821\n",
      "Data columns (total 7 columns):\n",
      "INVNUMBER      129822 non-null int64\n",
      "BILLDATE       129822 non-null int64\n",
      "INVDATE        129822 non-null int64\n",
      "INTDATE        129822 non-null int64\n",
      "PRODGRP        126313 non-null object\n",
      "SHIPWEIGHT     129822 non-null float64\n",
      "TOTALCHARGE    129822 non-null float64\n",
      "dtypes: float64(2), int64(4), object(1)\n",
      "memory usage: 6.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# set working directory & read in files\n",
    "wd = 'C:/Users/kkaufman/Documents/Data Sci/Intermediate/Freight model/'\n",
    "vol1 = pd.read_csv(wd + 'FY19outboundv2 - Q1.csv', low_memory=False)\n",
    "vol2 = pd.read_csv(wd + 'FY19outboundv2 - Q2.csv', low_memory=False)\n",
    "vol3 = pd.read_csv(wd + 'FY19outboundv2 - Q3.csv', low_memory=False)\n",
    "vol4 = pd.read_csv(wd + 'FY19outboundv2 - Q4.csv', low_memory=False)\n",
    "vol = vol1.append([vol2,vol3,vol4])\n",
    "freight = pd.read_csv(wd + 'FY19freightcharges.csv', low_memory=False) \n",
    "\n",
    "# print the summary information for each data frame\n",
    "print(vol.info())\n",
    "print(freight.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each dataset has been read in as a Pandas data frame.  Our consolidated volume data frame contains 22 variables and 1,131,645 observations, whereas our freight data frame contains 7 variables and 129,822 observations.  These shapes are relatively normal for a couple of reasons: first, the freight charges have been rolled up to the invoice level whereas the volume information is at a line level detail; second, not all of our outbound volume incurs freight cost to Wiley. So, we would expect our volume data to have many more observations than our freight data.\n",
    "\n",
    "The join keys for these two data frames will be the 3PL interface date (represented by CJDAT1 in vol and by INTDATE in freight) and the invoice number (represented by INVOICENO in vol and by INVNUMBER in freight).  *Note that invoice date functions differently in each dataset and cannot be used as a join key.*\n",
    "\n",
    "At the end of our data wrangling, we will need to roll up the volume dataset to an invoice level before joining in the freight cost data, as it is not correct to assume the freight cost for each invoice splits evenly among each line.\n",
    "\n",
    "\n",
    "### Data Types\n",
    "\n",
    "The attributes of the volume dataset include:\n",
    "* 3 Product description variables\n",
    "    + ISBN10, product line, global business (categorical; object)\n",
    "* 13 Invoice level variables\n",
    "    + Invoice date (IHINDT), 3PL interface date (CJDAT1), ship verify date (IHAEDT) (continuous; integer)\n",
    "    + Invoice number (continuous; integer)\n",
    "    + Batch number (categorical; object)\n",
    "    + Carton quantity, cartons per pallet (continuous; integer)\n",
    "    + Outbound units, outbound loose, outbound cartons, outbound pallets (continuous; integer)\n",
    "    + Book weight (continuous; float)\n",
    "* 2 Customer demographic variables\n",
    "    + Market outlet, customer destination (categorical; object)\n",
    "* 4 Transportion variables\n",
    "    + Ship method, freight type, priority code, carrier code, collect y/n (categorical; object)\n",
    "    \n",
    "The attributes of the freight dataset include:\n",
    "* 1 Product description variables\n",
    "    + Product group (categorical)\n",
    "* 5 Invoice level variables\n",
    "    + Invoice date (INVDATE), 3PL interface date (INTDATE), billing date (BILLDATE) (continuous; integer)\n",
    "    + Invoice number (INVNUMBER) (continuous; integer)\n",
    "    + Shipment weight (continuous; float)\n",
    "* 1 Output variable / target\n",
    "    + Total charge (continuous; float)\n",
    "    \n",
    "For the purposes of EDA and data wrangling, both of these datasets have been read in with the correct data types.  It is possible that when it is time to fit the model, we will need to convert our \"collect y/n\" field in the volume dataset to a binary 1/0, but this determination can be made later down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Luckily, the majority of the variables in both datasets are absent of missing values.  See below for a count of non-null observations to give a sense of this completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CJDAT1                1131645\n",
      "IHINDT                1131645\n",
      "IHAEDT                1131645\n",
      "INVOICENO             1131645\n",
      "ISBN10                1131645\n",
      "PRODUCT_LINE          1131645\n",
      "GLOBAL_BUSINESS       1131645\n",
      "BOOK_WEIGHT           1131645\n",
      "COLLECT_METHOD        1131645\n",
      "CARRIER_CODE          1131645\n",
      "PRIORITY_CODE         1131645\n",
      "SHIP_METHOD           1131645\n",
      "FREIGHT_TYPE          1048609\n",
      "SHIP_COUNTRY_NAME     1131632\n",
      "MARKET_OUTLET         1131645\n",
      "BATCH_NO              1131645\n",
      "CARTON_QTY            1131645\n",
      "CARTONS_PER_PALLET    1131645\n",
      "TOTAL_UNITS           1131645\n",
      "TOTAL_LOOSE           1131645\n",
      "TOTAL_CARTONS         1131645\n",
      "TOTAL_PALLETS         1131645\n",
      "dtype: int64\n",
      "INVNUMBER      129822\n",
      "BILLDATE       129822\n",
      "INVDATE        129822\n",
      "INTDATE        129822\n",
      "PRODGRP        126313\n",
      "SHIPWEIGHT     129822\n",
      "TOTALCHARGE    129822\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print a count of non-null values\n",
    "print(vol.count())\n",
    "print(freight.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our volume data is almost entirely clean aside from some missing values in the \"freight type\" variable.  Our freight type variable has approximately 83,000 null values, which represents about 7% of the total dataset.  We can refresh ourselves on the meaning of the freight type variable by taking a look at its unique values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subsidiary Ship Methods       ' 'Customer Pick Up              '\n",
      " 'GTW Prepaid Small Package     ' 'Pre-Paid Small Package        '\n",
      " 'Pre-Paid Air                  ' 'GTW Prepaid Freight           ' nan\n",
      " 'Collect Small Package         ' 'Collect Freight               '\n",
      " 'Pre-Paid Freight              ' 'Collect Air                   '\n",
      " 'GTW Collect Freight           ' \"Int'l Pre-Paid Air            \"\n",
      " 'GTW Collect Small Package     ' 'GTW Prepaid Air               '\n",
      " \"Int'l Collect Air             \" \"Int'l Collect Surface         \"\n",
      " 'GTW Collect Air               ' \"Int'l Pre-Paid Surface        \"]\n"
     ]
    }
   ],
   "source": [
    "print(vol['FREIGHT_TYPE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no immediately reliable way to make an assumption on what these missing values might be, we can impute them with a placeholder variable of \"Unknown.\"  It is also evident that the non-null freight type variables have a number of trailing spaces attached to the string, so we'll want to strip those spaces from the variable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1131645\n",
      "['Domestic Ground Small Package' 'Customer Pick Up'\n",
      " 'GTW Ground Small Package' 'Subsidiary Ship Methods' 'Unknown'\n",
      " 'GTW Ground Freight' 'Domestic Ground Freight' \"Int'l Air Courier\"\n",
      " 'Domestic Air Courier' 'GTW Air Courier' \"Int'l Surface\"\n",
      " 'Domestic Air Freight']\n"
     ]
    }
   ],
   "source": [
    "vol['FREIGHT_TYPE'] = vol['FREIGHT_TYPE'].str.strip().fillna('Unknown')\n",
    "print(vol['FREIGHT_TYPE'].count())\n",
    "print(vol['FREIGHT_TYPE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the missing freight type values, there are also exactly 13 missing values in our \"Ship Country Name\" field, which are negligible.  See below for the unique values of our Ship Country Name variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNITED STATES                 ' 'CANADA                        '\n",
      " 'UNITED KINGDOM                ' 'JAPAN                         '\n",
      " 'AUSTRALIA                     ' 'SINGAPORE                     '\n",
      " 'BRAZIL                        ' 'MALAYSIA                      '\n",
      " 'CHINA                         ' 'TAIWAN                        '\n",
      " 'INDIA                         ' 'ANTIGUA & BARBUDA             '\n",
      " 'SWEDEN                        ' 'FOREIGN PURCHASE  (USA)       '\n",
      " 'SOUTH KOREA                   ' 'NETHERLANDS                   '\n",
      " 'CHILE                         ' 'PHILIPPINES                   '\n",
      " 'BELGIUM                       ' 'ITALY                         '\n",
      " 'FRANCE                        ' 'MEXICO                        '\n",
      " 'POLAND                        ' 'GERMANY                       '\n",
      " 'ARGENTINA                     ' 'SWITZERLAND                   '\n",
      " 'VIETNAM                       ' 'PERU                          '\n",
      " 'COLOMBIA                      ' 'GREECE                        '\n",
      " 'ROMANIA                       ' 'INDONESIA                     '\n",
      " 'DENMARK                       ' 'PANAMA                        '\n",
      " 'RUSSIA                        ' 'SOUTH AFRICA                  '\n",
      " 'THAILAND                      ' 'KUWAIT                        '\n",
      " 'EGYPT                         ' 'AUSTRIA                       '\n",
      " 'HONG KONG                     ' 'VIRGIN ISLANDS (US)           '\n",
      " 'CZECH REPUBLIC                ' 'JAMAICA                       '\n",
      " 'KAZAKHSTAN                    ' 'GUAM                          '\n",
      " 'SPAIN                         ' 'PUERTO RICO                   '\n",
      " 'NEW ZEALAND                   ' 'PAKISTAN                      '\n",
      " 'VENEZUELA                     ' 'TRINIDAD AND TOBAGO           '\n",
      " 'IRELAND                       ' 'SLOVENIA                      '\n",
      " 'SAUDI ARABIA                  ' nan 'BARBADOS                      '\n",
      " 'TANZANIA                      ' 'CYPRUS                        '\n",
      " 'FINLAND                       ' 'PORTUGAL                      '\n",
      " 'ISRAEL                        ' 'MACEDONIA                     '\n",
      " 'URUGUAY                       ' 'JORDAN                        '\n",
      " 'TURKEY                        ' 'ALGERIA                       '\n",
      " 'QATAR                         ' 'UNITED ARAB EMIRATES          '\n",
      " 'NORWAY                        ' 'COSTA RICA                    '\n",
      " 'KENYA                         ' 'BERMUDA                       '\n",
      " 'CAMBODIA                      ' 'CROATIA                       '\n",
      " 'CURACAO                       ' 'CAYMAN ISLANDS                '\n",
      " 'BAHAMAS                       ' 'GHANA                         '\n",
      " 'ECUADOR                       ' 'IRAN                          '\n",
      " 'BAHRAIN                       ' 'SAINT KITTS AND NEVIS         '\n",
      " 'LIBYA                         ' 'LATVIA                        '\n",
      " 'ESTONIA                       ' 'US MINOR OUTLYING ISLANDS     '\n",
      " 'LUXEMBOURG                    ' 'ICELAND                       '\n",
      " 'GEORGIA                       ' 'SIERRA LEONE                  '\n",
      " 'BULGARIA                      ' 'HUNGARY                       '\n",
      " 'MOROCCO                       ' 'GRENADA                       '\n",
      " 'BRUNEI                        ' 'ZAMBIA                        '\n",
      " 'AFGHANISTAN                   ' 'BELARUS                       '\n",
      " 'BANGLADESH                    ' 'LEBANON                       '\n",
      " 'BELIZE                        ' 'MONACO                        '\n",
      " 'NIGERIA                       ' 'GREENLAND                     '\n",
      " 'SLOVAK REPUBLIC               ' 'MONGOLIA                      '\n",
      " 'UGANDA                        ' 'JERSEY                        '\n",
      " 'MALTA                         ' 'OMAN                          '\n",
      " 'EL SALVADOR                   ' 'ARUBA                         '\n",
      " 'SERBIA                        ' 'LITHUANIA                     '\n",
      " 'IRAQ                          ' 'UKRAINE                       '\n",
      " 'NAMIBIA                       ' 'BOLIVIA                       '\n",
      " 'SRI LANKA                     ' 'LIECHTENSTEIN                 '\n",
      " 'MYANMAR                       ' 'MAURITIUS                     '\n",
      " 'NICARAGUA                     ' 'SAIPAN                        '\n",
      " 'GUATEMALA                     ' 'Northern Mariana Islands      '\n",
      " 'DOMINICAN REPUBLIC            ' 'LIBERIA                       '\n",
      " 'SWAZILAND                     ' 'AZERBAIJAN                    '\n",
      " 'SAINT LUCIA                   ' 'BOTSWANA                      '\n",
      " 'SENEGAL                       ' 'GUERNSEY                      '\n",
      " 'HONDURAS                      ' 'GUADELOUPE                    '\n",
      " 'ZIMBABWE                      ' 'MOZAMBIQUE                    ']\n"
     ]
    }
   ],
   "source": [
    "print(vol['SHIP_COUNTRY_NAME'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may be able to derive the Ship Country Name from other variables as we get to know our data better.  In the meantime, we'll treat it similarly to freight type and impute the missing values with \"Unknown.\"  Since we can also see that the cases of the characters in the strings are inconsistent (see: \"Northern Mariana Islands\") and there are trailing spaces at the end, we'll want to clean up the non-null values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Singapore' 'United States' 'Canada' 'United Kingdom' 'Malaysia'\n",
      " 'Australia' 'India' 'Japan' 'China' 'South Korea' 'Brazil'\n",
      " 'Foreign Purchase  (Usa)' 'Taiwan' 'Puerto Rico' 'Germany'\n",
      " 'Trinidad And Tobago' 'Philippines' 'Greece' 'Vietnam' 'Italy' 'Russia'\n",
      " 'France' 'Peru' 'Thailand' 'Poland' 'Mexico' 'Jamaica' 'Netherlands'\n",
      " 'Chile' 'Latvia' 'Argentina' 'Grenada' 'Finland' 'Spain' 'Uruguay'\n",
      " 'Ecuador' 'Pakistan' 'Kuwait' 'Israel' 'Denmark' 'Antigua & Barbuda'\n",
      " 'Ghana' 'Romania' 'Switzerland' 'Belize' 'Sweden' 'New Zealand' 'Croatia'\n",
      " 'Czech Republic' 'Hong Kong' 'Barbados' 'Belgium' 'Cayman Islands'\n",
      " 'Unknown' 'Bulgaria' 'Saudi Arabia' 'Colombia' 'Norway' 'Costa Rica'\n",
      " 'Hungary' 'Venezuela' 'Tanzania' 'Slovenia' 'Ireland' 'Estonia'\n",
      " 'Portugal' 'Bahrain' 'Kazakhstan' 'Bermuda' 'South Africa' 'Austria'\n",
      " 'Egypt' 'United Arab Emirates' 'Qatar' 'Panama' 'Guam'\n",
      " 'Virgin Islands (Us)' 'Indonesia' 'Turkey' 'Iceland' 'Jordan'\n",
      " 'Afghanistan' 'Libya' 'Us Minor Outlying Islands' 'Bahamas' 'Georgia'\n",
      " 'Kenya' 'Lebanon' 'Morocco' 'Bangladesh' 'Monaco' 'Cambodia'\n",
      " 'Sierra Leone' 'Luxembourg' 'Brunei' 'Slovak Republic' 'Iran' 'Zambia'\n",
      " 'Algeria' 'Cyprus' 'Curacao' 'Belarus' 'Macedonia' 'Nigeria' 'Greenland'\n",
      " 'Saint Kitts And Nevis' 'Mongolia' 'Iraq' 'Bolivia' 'Uganda' 'Oman'\n",
      " 'El Salvador' 'Malta' 'Namibia' 'Serbia' 'Sri Lanka' 'Swaziland'\n",
      " 'Botswana' 'Mozambique' 'Aruba' 'Liechtenstein' 'Lithuania' 'Mauritius'\n",
      " 'Northern Mariana Islands' 'Saint Lucia' 'Azerbaijan' 'Ukraine'\n",
      " 'Honduras' 'Senegal' 'Saipan' 'Jersey' 'Myanmar' 'Zimbabwe' 'Guatemala'\n",
      " 'Liberia' 'Nicaragua' 'Dominican Republic' 'Guernsey' 'Guadeloupe']\n"
     ]
    }
   ],
   "source": [
    "vol['SHIP_COUNTRY_NAME'] = vol['SHIP_COUNTRY_NAME'].str.title().str.strip().fillna('Unknown')\n",
    "vol['SHIP_METHOD'] = vol['SHIP_METHOD'].str.strip()\n",
    "print(vol['SHIP_COUNTRY_NAME'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that other text string fields in the volume dataset, such as \"Ship Method,\" have been cleansed of trailing spaces as well.\n",
    "\n",
    "Our freight dataset is entirely absent of missing values aside from the \"Product Group\" field.  \"Product Group\" is a 3PL created identifier that distinguishes between our test prep product and non-test prep product, but has only been implemented as a field several months ago (hence the null values).  Due to the incomplete nature of this field as well as the fact that it is a redundant variable (it is repeated in more detail by the \"Global Business\" field in the volume dataset), we can safely assume this field will not be used in the model and that it can therefore be left alone in terms of missing value treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Outlier treatment for these datasets will depend on the statistical summary of our variables.  See below for summary stats on each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        BOOK_WEIGHT  TOTAL_PALLETS  TOTAL_CARTONS   TOTAL_LOOSE   TOTAL_UNITS\n",
      "count  1.131645e+06   1.131645e+06   1.131645e+06  1.131645e+06  1.131645e+06\n",
      "mean   2.902273e+01   2.301959e-03   4.131746e-01  2.593339e+00  1.126777e+01\n",
      "std    2.730440e+01   1.700169e-01   2.357927e+00  5.239535e+00  1.130221e+02\n",
      "min    0.000000e+00   0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00\n",
      "25%    1.600000e+01   0.000000e+00   0.000000e+00  1.000000e+00  1.000000e+00\n",
      "50%    2.080000e+01   0.000000e+00   0.000000e+00  1.000000e+00  1.000000e+00\n",
      "75%    3.760000e+01   0.000000e+00   0.000000e+00  2.000000e+00  4.000000e+00\n",
      "max    9.037440e+03   8.600000e+01   1.200000e+02  2.400000e+03  3.825400e+04\n",
      "          SHIPWEIGHT    TOTALCHARGE\n",
      "count  129822.000000  129822.000000\n",
      "mean       36.403733      15.069798\n",
      "std       664.402469      56.250573\n",
      "min         0.000000    -241.040000\n",
      "25%         2.200000       5.071500\n",
      "50%         3.600000       6.980500\n",
      "75%         7.000000       9.982000\n",
      "max     90978.000000    4665.136000\n"
     ]
    }
   ],
   "source": [
    "print(vol.loc[:,['BOOK_WEIGHT','TOTAL_PALLETS','TOTAL_CARTONS','TOTAL_LOOSE','TOTAL_UNITS']].describe())\n",
    "print(freight.iloc[:,5:7].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all continuous variables that are meant to function as numbers rather than identifiers (e.g. all continuous variables aside from invoice number and dates), distributions of data have been checked for outliers.  Although it cannot necessarily be intuited from the summary statistics, several of these variables have noticeable outliers.\n",
    "\n",
    "One of these variables is book weight, which appears to have a few (3) outliers above 8,000 ounces.  We can also see that 11,523 values are equal to zero, which should not be the case for a physical item.  For this variable, we will want to take a capping/flooring approach to outlier handling by imputing all low and high outliers with the 5th/95th percentiles, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book weight by percentile\n",
      "0.00       0.00\n",
      "0.05       6.88\n",
      "0.10      10.40\n",
      "0.20      14.40\n",
      "0.30      16.80\n",
      "0.40      18.40\n",
      "0.50      20.80\n",
      "0.60      25.60\n",
      "0.70      32.80\n",
      "0.80      44.00\n",
      "0.90      53.60\n",
      "0.95      68.80\n",
      "1.00    9037.44\n",
      "Name: BOOK_WEIGHT, dtype: float64\n",
      "Book weight outliers > 8,000 and equal to 0\n",
      "3\n",
      "11523\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "vol['BOOK_WEIGHT'].plot(kind= 'box')\n",
    "plt.show()\n",
    "print('Book weight by percentile')\n",
    "print(vol['BOOK_WEIGHT'].quantile([0,.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,.95,1]))\n",
    "print('Book weight outliers > 8,000 and equal to 0')\n",
    "print((vol.loc[vol['BOOK_WEIGHT']>8000])['BOOK_WEIGHT'].count())\n",
    "print((vol.loc[vol['BOOK_WEIGHT']==0])['BOOK_WEIGHT'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to book weight, we also have to check for outliers in our total units, total loose, total cartons, and total pallets fields.  It is worth noting that we will not be inspecting our carton quantity or cartons per pallet fields, since those fields cannot be aggregated to an invoice level and mainly exist to calculate cartons and pallets for each line.\n",
    "\n",
    "Although there are some zeroes in our pallets, cartons, and loose fields, this is normal and to be expected when translating unit numbers into pallets, cartons, and loose based on their dimensions.  There should not be zeroes in our total units column, but since this only represents 9 observations, it should be okay to leave them in the dataset.\n",
    "\n",
    "The same conclusion can be made for our outliers on the high side (above the 95th percentile).  Although these numbers are quite high, they do represent legitimately large shipments, and should therefore not be altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TOTAL_PALLETS  TOTAL_CARTONS  TOTAL_LOOSE  TOTAL_UNITS\n",
      "0.00            0.0            0.0          0.0          0.0\n",
      "0.05            0.0            0.0          0.0          1.0\n",
      "0.10            0.0            0.0          1.0          1.0\n",
      "0.20            0.0            0.0          1.0          1.0\n",
      "0.30            0.0            0.0          1.0          1.0\n",
      "0.40            0.0            0.0          1.0          1.0\n",
      "0.50            0.0            0.0          1.0          1.0\n",
      "0.60            0.0            0.0          1.0          2.0\n",
      "0.70            0.0            0.0          2.0          3.0\n",
      "0.80            0.0            0.0          3.0          6.0\n",
      "0.90            0.0            1.0          6.0         20.0\n",
      "0.95            0.0            2.0         10.0         36.0\n",
      "1.00           86.0          120.0       2400.0      38254.0\n"
     ]
    }
   ],
   "source": [
    "print(vol[['TOTAL_PALLETS','TOTAL_CARTONS','TOTAL_LOOSE','TOTAL_UNITS']].quantile([0,.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,.95,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Datasets\n",
    "\n",
    "Now that the data has been cleaned and checked for outliers, the volume data must be rolled up to an invoice level and then joined to the freight data.  We will do this by grouping by our invoice numbers, interface dates, and categorical variables, and then summarizing the continuous volume variables such as total lines, units, loose units, cartons, pallets, and weight before joining to the freight charge data.\n",
    "\n",
    "One slight complication to our volume data involves categorizing invoices by the owner of the product.  In this dataset, we have 31,165 unique ISBN10s, each of which rolls up to 71 unique product lines that indicate the type of product (Dummies books, Chemistry textbooks, etc) and each of these product lines rolls up to one of 5 global businesses (Trade, Education, Reference, Test Prep, and Agency).  Due to the fact that we regularly ship orders with multiple types of product in one shipment, we will need to make adjustments to allocate a single global business to each invoice.\n",
    "\n",
    "Although a number of factors go into costing a shipment, it is safe to assume that the most prominent driver of cost is the weight of the shipment.  Thus, we will build a separate data frame that summarizes our invoice numbers and global businesses by the sum of our book weight variable, and then we can select the global business with the maximum weight in each shipment.  Although this methodology does not give us an exact allocation of global business product, it will give us the best possible summary of our freight and volume data without duplicating observations or attempting to allocate our numerical variables in a way that is not necessarily in line with the higher invoice-level costing of freight charges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    INVOICENO GLOBAL_BUSINESS  BOOK_WEIGHT\n",
      "0      153991       Test Prep        52.00\n",
      "1      153992           Trade        27.20\n",
      "2      153993           Trade        33.60\n",
      "3      154003           Trade         8.00\n",
      "4      154007           Trade        35.20\n",
      "5      154008           Trade       176.32\n",
      "6      154009          Agency        56.80\n",
      "7      154009       Education        64.80\n",
      "8      154009       Reference       284.64\n",
      "9      154009           Trade        55.84\n",
      "10     154010       Reference       212.00\n",
      "11     154011       Reference        34.40\n",
      "12     154011           Trade       173.60\n",
      "13     154012       Education        25.28\n",
      "14     154013       Education        36.80\n"
     ]
    }
   ],
   "source": [
    "wgtidx = vol.groupby(['INVOICENO','GLOBAL_BUSINESS'])['BOOK_WEIGHT'].sum()\n",
    "wgtidx = pd.DataFrame(wgtidx)\n",
    "wgtidx.reset_index(inplace=True)\n",
    "print(wgtidx.head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
